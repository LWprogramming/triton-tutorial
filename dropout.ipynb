{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7697bff8-7d91-43c8-b4fc-bc08920dd171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting triton\n",
      "  Downloading triton-2.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (63.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.2/63.2 MB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tabulate\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Requirement already satisfied: pandas in ./.local/lib/python3.8/site-packages (1.5.1)\n",
      "Requirement already satisfied: matplotlib in ./.local/lib/python3.8/site-packages (3.5.3)\n",
      "Collecting cmake\n",
      "  Downloading cmake-3.25.2-py2.py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m102.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch in /usr/lib/python3/dist-packages (from triton) (1.12.1)\n",
      "Collecting lit\n",
      "  Downloading lit-15.0.7.tar.gz (132 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.3/132.3 kB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/lib/python3/dist-packages (from triton) (3.0.12)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.local/lib/python3.8/site-packages (from pandas) (2022.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in ./.local/lib/python3.8/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.20.3 in ./.local/lib/python3.8/site-packages (from pandas) (1.23.4)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.6)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/lib/python3/dist-packages (from matplotlib) (1.0.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.local/lib/python3.8/site-packages (from matplotlib) (4.38.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/lib/python3/dist-packages (from matplotlib) (7.0.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/lib/python3/dist-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.local/lib/python3.8/site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas) (1.14.0)\n",
      "Building wheels for collected packages: lit\n",
      "  Building wheel for lit (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for lit: filename=lit-15.0.7-py3-none-any.whl size=89984 sha256=aafbc7abe30ad149811ac45290fbadb5b520a7c655650c28db68a9ca390cba64\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/50/6e/bd/151b1c4a1f8bcedef7c39a14a956730a7c3185c62754d119d0\n",
      "Successfully built lit\n",
      "Installing collected packages: lit, cmake, triton, tabulate\n",
      "Successfully installed cmake-3.25.2 lit-15.0.7 tabulate-0.9.0 triton-2.0.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install triton tabulate pandas matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04add3b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------  --------  --------  ---------  --------  -------  ---------  --------  ---------  --------  -------\n",
      "input      -1.75378  0.495332  -0.793169  -1.17336  2.27975  0.0690164  0.410273  -0.796761  0.769278  0.55273\n",
      "keep mask   0        0          1          0        0        0          0          1         1         0\n",
      "output      0        0         -1.58634    0        0        0          0         -1.59352   1.53856   0\n",
      "---------  --------  --------  ---------  --------  -------  ---------  --------  ---------  --------  -------\n"
     ]
    }
   ],
   "source": [
    "import tabulate\n",
    "import torch\n",
    "\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def _dropout(\n",
    "        x_ptr,  # pointer to the input\n",
    "        x_keep_ptr,  # pointer to a mask of 0s and 1s\n",
    "        output_ptr,  # pointer to the output\n",
    "        n_elements,  # number of elements in the `x` tensor\n",
    "        p,  # probability that an element of `x` is changed to zero\n",
    "        BLOCK_SIZE: tl.constexpr,\n",
    "):\n",
    "    pid = tl.program_id(axis=0)\n",
    "    block_start = pid * BLOCK_SIZE\n",
    "    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n",
    "    mask = offsets < n_elements\n",
    "    # Load data\n",
    "    x = tl.load(x_ptr + offsets, mask=mask)\n",
    "    x_keep = tl.load(x_keep_ptr + offsets, mask=mask)\n",
    "    # The line below is the crucial part, described in the paragraph above!\n",
    "    output = tl.where(x_keep, x / (1 - p), 0.0)\n",
    "    # Write-back output\n",
    "    tl.store(output_ptr + offsets, output, mask=mask)\n",
    "\n",
    "\n",
    "def dropout(x, x_keep, p):\n",
    "    output = torch.empty_like(x)\n",
    "    assert x.is_contiguous()\n",
    "    n_elements = x.numel()\n",
    "    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n",
    "    _dropout[grid](x, x_keep, output, n_elements, p, BLOCK_SIZE=1024)\n",
    "    return output\n",
    "\n",
    "\n",
    "# Input tensor\n",
    "x = torch.randn(size=(10,)).cuda()\n",
    "# Dropout mask\n",
    "p = 0.5\n",
    "x_keep = (torch.rand(size=(10,)) > p).to(torch.int32).cuda()\n",
    "#\n",
    "output = dropout(x, x_keep=x_keep, p=p)\n",
    "print(tabulate.tabulate([\n",
    "    [\"input\"] + x.tolist(),\n",
    "    [\"keep mask\"] + x_keep.tolist(),\n",
    "    [\"output\"] + output.tolist()\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9451619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------  --------  -------  ---------  -------  ---------  ---------  --------  --------  --------  --------\n",
      "input                0.503197  1.18018  -0.714294  1.78129  -0.106496  -0.174402  0.734475  0.350418  0.307363  -1.54676\n",
      "output (seed = 123)  0         2.36036   0         0         0         -0.348805  0         0         0.614726  -3.09352\n",
      "output (seed = 123)  0         2.36036   0         0         0         -0.348805  0         0         0.614726  -3.09352\n",
      "output (seed = 512)  0         0        -1.42859   3.56258   0         -0.348805  1.46895   0         0          0\n",
      "-------------------  --------  -------  ---------  -------  ---------  ---------  --------  --------  --------  --------\n"
     ]
    }
   ],
   "source": [
    "@triton.jit\n",
    "def _seeded_dropout(\n",
    "        x_ptr,\n",
    "        output_ptr,\n",
    "        n_elements,\n",
    "        p,\n",
    "        seed,\n",
    "        BLOCK_SIZE: tl.constexpr,\n",
    "):\n",
    "    # compute memory offsets of elements handled by this instance\n",
    "    pid = tl.program_id(axis=0)\n",
    "    block_start = pid * BLOCK_SIZE\n",
    "    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n",
    "    # load data from x\n",
    "    mask = offsets < n_elements\n",
    "    x = tl.load(x_ptr + offsets, mask=mask)\n",
    "    # randomly prune it\n",
    "    random = tl.rand(seed, offsets)\n",
    "    x_keep = random > p\n",
    "    # write-back\n",
    "    output = tl.where(x_keep, x / (1 - p), 0.0)\n",
    "    tl.store(output_ptr + offsets, output, mask=mask)\n",
    "\n",
    "\n",
    "def seeded_dropout(x, p, seed):\n",
    "    output = torch.empty_like(x)\n",
    "    assert x.is_contiguous()\n",
    "    n_elements = x.numel()\n",
    "    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n",
    "    _seeded_dropout[grid](x, output, n_elements, p, seed, BLOCK_SIZE=1024)\n",
    "    return output\n",
    "\n",
    "\n",
    "x = torch.randn(size=(10,)).cuda()\n",
    "# Compare this to the baseline - dropout mask is never instantiated!\n",
    "output = seeded_dropout(x, p=0.5, seed=123)\n",
    "output2 = seeded_dropout(x, p=0.5, seed=123)\n",
    "output3 = seeded_dropout(x, p=0.5, seed=512)\n",
    "\n",
    "print(tabulate.tabulate([\n",
    "    [\"input\"] + x.tolist(),\n",
    "    [\"output (seed = 123)\"] + output.tolist(),\n",
    "    [\"output (seed = 123)\"] + output2.tolist(),\n",
    "    [\"output (seed = 512)\"] + output3.tolist()\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "01d956b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------  -------------------------------------------  ----------------------------------------  -----------------------------------------\n",
      "input                        [-0.8444323539733887, 0.004563711117953062]  [-1.417240858078003, 0.5809898972511292]  [0.4518235921859741, 0.47092223167419434]\n",
      "output (first set of seeds)  [-1.6888647079467773, 0.009127422235906124]  [-2.834481716156006, 1.1619797945022583]  [0.9036471843719482, 0.9418444633483887]\n",
      "output (2nd set)             [-1.6888647079467773, 0.009127422235906124]  [-2.834481716156006, 1.1619797945022583]  [0.9036471843719482, 0.9418444633483887]\n",
      "output (3rd set)             [-1.6888647079467773, 0.009127422235906124]  [-2.834481716156006, 1.1619797945022583]  [0.9036471843719482, 0.9418444633483887]\n",
      "---------------------------  -------------------------------------------  ----------------------------------------  -----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1: dropout for matrix with vector of seeds, 1 seed per row\n",
    "@triton.jit\n",
    "def _seeded_matrix_dropout(\n",
    "        x_ptr,\n",
    "        output_ptr,\n",
    "        n_elements,\n",
    "        p,\n",
    "        seeds,\n",
    "        n_elements_per_row: tl.constexpr,\n",
    "        BLOCK_SIZE: tl.constexpr,\n",
    "):\n",
    "    # compute memory offsets of elements handled by this instance\n",
    "    pid = tl.program_id(axis=0)\n",
    "    # index of first row in block.\n",
    "    # e.g. if BLOCK_SIZE is 1024 and this is block 3, then the first row is row 3 * 1024 in the original tensor\n",
    "    start_row_index = pid * BLOCK_SIZE\n",
    "    block_start = start_row_index * n_elements_per_row\n",
    "\n",
    "    # offsets is now a BLOCK_SIZE x n_elements_per_row matrix\n",
    "    row_offsets = (tl.arange(0, BLOCK_SIZE) * n_elements_per_row)[:, None] # left operand is how many rows down we go in current block, right is num elements for that row\n",
    "    col_offsets = tl.arange(0, n_elements_per_row)[None, :]\n",
    "    offsets = block_start + row_offsets + col_offsets\n",
    "\n",
    "    # load data from x\n",
    "    mask = offsets < n_elements\n",
    "    x = tl.load(x_ptr + offsets, mask=mask)\n",
    "    # randomly prune it\n",
    "    # my_seed = 13\n",
    "    random_values = tl.zeros((BLOCK_SIZE, n_elements_per_row), dtype=tl.float32) # one random value per row\n",
    "    for row in range(0, BLOCK_SIZE, 1):\n",
    "        # note that we get seed from start_row_index + row, NOT row by itself\n",
    "        # we need start_row_index to get the right index into seeds vector.\n",
    "\n",
    "        # random_mask one-hot for current row\n",
    "        # offsets % block_start effectively makes offsets like a matrix [[0, 1, 2, ... n_elements_per_row - 1], [n_elements_per_row, etc]]\n",
    "        # then just accept where offsets / n_elements_per_row == row\n",
    "        random_mask = tl.where((offsets % block_start) / n_elements_per_row, 1.0, 0.0)\n",
    "        # TODO: not sure how exactly to do this? idea is to select the seed from a specific index in seeds\n",
    "        # but that fails with basically no error message besides\n",
    "        # ValueError: Did you forget to add @triton.jit ? (`_builder` argument must be provided outside of JIT functions.)\n",
    "        # which is wrong obviously.     \n",
    "        my_seed = seeds\n",
    "        random_values += tl.rand(my_seed, random_values) * random_mask\n",
    "        # random_values += tl.rand(seeds[start_row_index + row], random_values) * random_mask\n",
    "        # random_values[row] = tl.rand(seeds[start_row_index + row], col_offsets) # fill out one row at a time just isn't supported\n",
    "    # random = tl.rand(my_seed, offsets)\n",
    "    x_keep = random_values > p\n",
    "    # write-back\n",
    "    output = tl.where(x_keep, x / (1 - p), 0.0)\n",
    "    tl.store(output_ptr + offsets, output, mask=mask)\n",
    "\n",
    "\n",
    "def seeded_matrix_dropout(x, p, seeds):\n",
    "    output = torch.empty_like(x)\n",
    "    assert x.is_contiguous()\n",
    "    assert seeds.shape == (x.shape[0],), f\"seeds should be length of num rows but instead got seeds.shape {seeds.shape} and x.shape {x.shape}\"\n",
    "    # raise AssertionError(\"ok\")\n",
    "    n_elements = x.numel()\n",
    "    # print(triton.cdiv(n_elements, 1024))\n",
    "    # print(n_elements)\n",
    "    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),) # for now, treat BLOCK_SIZE as the number of rows per block.\n",
    "    n_elements_per_row = x.shape[1]\n",
    "    _seeded_matrix_dropout[grid](x, output, n_elements, p, seeds, n_elements_per_row, BLOCK_SIZE=1024)\n",
    "    return output\n",
    "\n",
    "x = torch.randn(size=(3, 2)).cuda()\n",
    "# Compare this to the baseline - dropout mask is never instantiated!\n",
    "output = seeded_matrix_dropout(x, p=0.5, seeds=torch.tensor([123, 101, 1]).cuda())\n",
    "output2 = seeded_matrix_dropout(x, p=0.5, seeds=torch.tensor([123, 101, 2]).cuda())\n",
    "output3 = seeded_matrix_dropout(x, p=0.5, seeds=torch.tensor([512, 101, 3]).cuda())\n",
    "\n",
    "print(tabulate.tabulate([\n",
    "    [\"input\"] + x.tolist(),\n",
    "    [\"output (first set of seeds)\"] + output.tolist(),\n",
    "    [\"output (2nd set)\"] + output2.tolist(),\n",
    "    [\"output (3rd set)\"] + output3.tolist()\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "73a60f90-aaad-40ed-82b3-c4a44dc77aa8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,     1,     2,  ...,     7,     8,     9],\n",
       "        [   10,    11,    12,  ...,    17,    18,    19],\n",
       "        [   20,    21,    22,  ...,    27,    28,    29],\n",
       "        ...,\n",
       "        [10210, 10211, 10212,  ..., 10217, 10218, 10219],\n",
       "        [10220, 10221, 10222,  ..., 10227, 10228, 10229],\n",
       "        [10230, 10231, 10232,  ..., 10237, 10238, 10239]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 1024\n",
    "n_elements_per_row = 10\n",
    "n_elements_per_row * torch.arange(0, 1024)[:, None] + torch.arange(0, n_elements_per_row)[None, :]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
