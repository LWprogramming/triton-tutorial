{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7697bff8-7d91-43c8-b4fc-bc08920dd171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: triton in ./.local/lib/python3.8/site-packages (2.0.0.post1)\n",
      "Requirement already satisfied: tabulate in ./.local/lib/python3.8/site-packages (0.9.0)\n",
      "Requirement already satisfied: pandas in ./.local/lib/python3.8/site-packages (1.5.1)\n",
      "Requirement already satisfied: matplotlib in ./.local/lib/python3.8/site-packages (3.5.3)\n",
      "Requirement already satisfied: cmake in ./.local/lib/python3.8/site-packages (from triton) (3.26.0)\n",
      "Requirement already satisfied: lit in ./.local/lib/python3.8/site-packages (from triton) (16.0.0)\n",
      "Requirement already satisfied: torch in /usr/lib/python3/dist-packages (from triton) (1.12.1)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from triton) (3.0.12)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.local/lib/python3.8/site-packages (from pandas) (2022.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in ./.local/lib/python3.8/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.20.3 in ./.local/lib/python3.8/site-packages (from pandas) (1.23.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/lib/python3/dist-packages (from matplotlib) (7.0.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/lib/python3/dist-packages (from matplotlib) (1.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.local/lib/python3.8/site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/lib/python3/dist-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.6)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.local/lib/python3.8/site-packages (from matplotlib) (4.38.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas) (1.14.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install triton tabulate pandas matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04add3b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------  ---------  --------  ---------  -------  --------  --------  -------  ---------  ---------  -------\n",
      "input      0.0185069  -1.49556  0.0994412  1.10682  0.193011  0.279261  1.63456  -0.169297  -0.602044  1.17376\n",
      "keep mask  0           1        1          1        1         1         1         0          1         1\n",
      "output     0          -2.99111  0.198882   2.21363  0.386023  0.558523  3.26912   0         -1.20409   2.34752\n",
      "---------  ---------  --------  ---------  -------  --------  --------  -------  ---------  ---------  -------\n"
     ]
    }
   ],
   "source": [
    "import tabulate\n",
    "import torch\n",
    "\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def _dropout(\n",
    "        x_ptr,  # pointer to the input\n",
    "        x_keep_ptr,  # pointer to a mask of 0s and 1s\n",
    "        output_ptr,  # pointer to the output\n",
    "        n_elements,  # number of elements in the `x` tensor\n",
    "        p,  # probability that an element of `x` is changed to zero\n",
    "        BLOCK_SIZE: tl.constexpr,\n",
    "):\n",
    "    pid = tl.program_id(axis=0)\n",
    "    block_start = pid * BLOCK_SIZE\n",
    "    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n",
    "    mask = offsets < n_elements\n",
    "    # Load data\n",
    "    x = tl.load(x_ptr + offsets, mask=mask)\n",
    "    x_keep = tl.load(x_keep_ptr + offsets, mask=mask)\n",
    "    # The line below is the crucial part, described in the paragraph above!\n",
    "    output = tl.where(x_keep, x / (1 - p), 0.0)\n",
    "    # Write-back output\n",
    "    tl.store(output_ptr + offsets, output, mask=mask)\n",
    "\n",
    "\n",
    "def dropout(x, x_keep, p):\n",
    "    output = torch.empty_like(x)\n",
    "    assert x.is_contiguous()\n",
    "    n_elements = x.numel()\n",
    "    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n",
    "    _dropout[grid](x, x_keep, output, n_elements, p, BLOCK_SIZE=1024)\n",
    "    return output\n",
    "\n",
    "\n",
    "# Input tensor\n",
    "x = torch.randn(size=(10,)).cuda()\n",
    "# Dropout mask\n",
    "p = 0.5\n",
    "x_keep = (torch.rand(size=(10,)) > p).to(torch.int32).cuda()\n",
    "#\n",
    "output = dropout(x, x_keep=x_keep, p=p)\n",
    "print(tabulate.tabulate([\n",
    "    [\"input\"] + x.tolist(),\n",
    "    [\"keep mask\"] + x_keep.tolist(),\n",
    "    [\"output\"] + output.tolist()\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9451619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------  ---------  --------  -------  ---------  ----------  --------  --------  --------  ---------  ----------\n",
      "input                -0.957525  -1.15484  1.08971  -0.288357  -0.0398638  -1.21333  0.159923  -1.41682  -0.16531   -0.0420467\n",
      "output (seed = 123)   0         -2.30969  0         0          0          -2.42667  0          0        -0.330619  -0.0840935\n",
      "output (seed = 123)   0         -2.30969  0         0          0          -2.42667  0          0        -0.330619  -0.0840935\n",
      "output (seed = 512)   0          0        2.17943  -0.576715   0          -2.42667  0.319847   0         0          0\n",
      "-------------------  ---------  --------  -------  ---------  ----------  --------  --------  --------  ---------  ----------\n"
     ]
    }
   ],
   "source": [
    "@triton.jit\n",
    "def _seeded_dropout(\n",
    "        x_ptr,\n",
    "        output_ptr,\n",
    "        n_elements,\n",
    "        p,\n",
    "        seed,\n",
    "        BLOCK_SIZE: tl.constexpr,\n",
    "):\n",
    "    # compute memory offsets of elements handled by this instance\n",
    "    pid = tl.program_id(axis=0)\n",
    "    block_start = pid * BLOCK_SIZE\n",
    "    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n",
    "    # load data from x\n",
    "    mask = offsets < n_elements\n",
    "    x = tl.load(x_ptr + offsets, mask=mask)\n",
    "    # randomly prune it\n",
    "    random = tl.rand(seed, offsets)\n",
    "    x_keep = random > p\n",
    "    # write-back\n",
    "    output = tl.where(x_keep, x / (1 - p), 0.0)\n",
    "    tl.store(output_ptr + offsets, output, mask=mask)\n",
    "\n",
    "\n",
    "def seeded_dropout(x, p, seed):\n",
    "    output = torch.empty_like(x)\n",
    "    assert x.is_contiguous()\n",
    "    n_elements = x.numel()\n",
    "    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n",
    "    _seeded_dropout[grid](x, output, n_elements, p, seed, BLOCK_SIZE=1024)\n",
    "    return output\n",
    "\n",
    "\n",
    "x = torch.randn(size=(10,)).cuda()\n",
    "# Compare this to the baseline - dropout mask is never instantiated!\n",
    "output = seeded_dropout(x, p=0.5, seed=123)\n",
    "output2 = seeded_dropout(x, p=0.5, seed=123)\n",
    "output3 = seeded_dropout(x, p=0.5, seed=512)\n",
    "\n",
    "print(tabulate.tabulate([\n",
    "    [\"input\"] + x.tolist(),\n",
    "    [\"output (seed = 123)\"] + output.tolist(),\n",
    "    [\"output (seed = 123)\"] + output2.tolist(),\n",
    "    [\"output (seed = 512)\"] + output3.tolist()\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01d956b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------  ------------------------------------------------------------------------------------  -----------------------------------------------------------------------------------  ---------------------------------------------------------------------------------  -  -  --------  -  -  ---------  ----------\n",
      "input                        [-1.3508756160736084, 0.11341709643602371, -0.24785758554935455, 1.0647841691970825]  [-2.004702091217041, -0.03761491924524307, 1.1699249744415283, -2.3878366947174072]  [1.4394214153289795, 1.8197963237762451, 0.48485055565834045, 0.5667889714241028]\n",
      "output (first set of seeds)  0.0                                                                                   -2.3096888065338135                                                                  0.0                                                                                0  0  -2.42667  0  0  -0.330619  -0.0840935\n",
      "output (2nd set)             [0.0, 0.0, 0.0, 0.0]                                                                  [-4.009404182434082, -0.07522983849048615, 2.3398499488830566, -4.7756733894348145]  [2.878842830657959, 3.6395926475524902, 0.9697011113166809, 1.1335779428482056]\n",
      "output (3rd set)             [0.0, 0.0, 0.0, 0.0]                                                                  [-4.009404182434082, -0.07522983849048615, 2.3398499488830566, -4.7756733894348145]  [2.878842830657959, 3.6395926475524902, 0.9697011113166809, 1.1335779428482056]\n",
      "---------------------------  ------------------------------------------------------------------------------------  -----------------------------------------------------------------------------------  ---------------------------------------------------------------------------------  -  -  --------  -  -  ---------  ----------\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-f1c16ddc7a0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0mrandom_mask_should_be\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_mask_should_be\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug_random_mask_ptr3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Exercise 1: dropout for matrix with vector of seeds, 1 seed per row\n",
    "@triton.jit\n",
    "def _seeded_matrix_dropout(\n",
    "        x_ptr,\n",
    "        output_ptr,\n",
    "        debug_random_mask_ptr,\n",
    "        n_elements,\n",
    "        p,\n",
    "        seeds,\n",
    "        n_elements_per_row: tl.constexpr,\n",
    "        BLOCK_SIZE: tl.constexpr,\n",
    "):\n",
    "    # compute memory offsets of elements handled by this instance\n",
    "    pid = tl.program_id(axis=0)\n",
    "    # index of first row in block.\n",
    "    # e.g. if BLOCK_SIZE is 1024 and this is block 3, then the first row is row 3 * 1024 in the original tensor\n",
    "    start_row_index = pid * BLOCK_SIZE\n",
    "    block_start = start_row_index * n_elements_per_row\n",
    "\n",
    "    # offsets is now a BLOCK_SIZE x n_elements_per_row matrix\n",
    "    row_offsets = (tl.arange(0, BLOCK_SIZE) * n_elements_per_row)[:, None] # left operand is how many rows down we go in current block, right is num elements for that row\n",
    "    col_offsets = tl.arange(0, n_elements_per_row)[None, :]\n",
    "    offsets = block_start + row_offsets + col_offsets\n",
    "\n",
    "    # load data from x\n",
    "    mask = offsets < n_elements\n",
    "    x = tl.load(x_ptr + offsets, mask=mask)\n",
    "    # randomly prune it\n",
    "    random_values = tl.zeros((BLOCK_SIZE, n_elements_per_row), dtype=tl.float32) # one random value per row\n",
    "    for row in range(BLOCK_SIZE):\n",
    "        # note that we get seed from start_row_index + row, NOT row by itself\n",
    "        # we need start_row_index to get the right index into seeds vector.\n",
    "\n",
    "        # random_mask one-hot for current row\n",
    "        # offsets % block_start effectively makes offsets like a matrix [[0, 1, 2, ... n_elements_per_row - 1], [n_elements_per_row, etc]]\n",
    "        # then just accept where offsets / n_elements_per_row == row\n",
    "        random_mask = tl.where(((row_offsets + col_offsets) // n_elements_per_row) == row, 1.0, 0.0)\n",
    "        if row == 1:\n",
    "            tl.store(debug_random_mask_ptr + row_offsets + col_offsets, random_mask, mask=mask)\n",
    "        # TODO: ?? why does offsets % block_start not work????\n",
    "        # random_mask = tl.where(((offsets % block_start) // n_elements_per_row) == row, 1.0, 0.0)\n",
    "        # random_mask = tl.where(((offsets % block_start) // n_elements_per_row) == 1, 1.0, 0.0)\n",
    "        # TODO: not sure how exactly to do this? idea is to select the seed from a specific index in seeds\n",
    "        # but that fails with basically no error message besides\n",
    "        # ValueError: Did you forget to add @triton.jit ? (`_builder` argument must be provided outside of JIT functions.)\n",
    "        # which is wrong obviously.     \n",
    "        random_values += (tl.rand(seeds + start_row_index + row, random_values) * random_mask)\n",
    "        # random_values += tl.rand(seeds[start_row_index + row], random_values) * random_mask\n",
    "        # random_values[row] = tl.rand(seeds[start_row_index + row], col_offsets) # fill out one row at a time just isn't supported\n",
    "    # random = tl.rand(my_seed, offsets)\n",
    "    x_keep = random_values > p\n",
    "    # write-back\n",
    "    output = tl.where(x_keep, x / (1 - p), 0.0)\n",
    "    tl.store(output_ptr + offsets, output, mask=mask)\n",
    "\n",
    "\n",
    "def seeded_matrix_dropout(x, p, seeds):\n",
    "    output = torch.empty_like(x)\n",
    "    assert x.is_contiguous()\n",
    "    assert seeds.shape == (x.shape[0],), f\"seeds should be length of num rows but instead got seeds.shape {seeds.shape} and x.shape {x.shape}\"\n",
    "    # raise AssertionError(\"ok\")\n",
    "    n_elements = x.numel()\n",
    "    # print(triton.cdiv(n_elements, 1024))\n",
    "    # print(n_elements)\n",
    "    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),) # for now, treat BLOCK_SIZE as the number of rows per block.\n",
    "    n_elements_per_row = x.shape[1]\n",
    "    debug_random_mask_ptr = torch.zeros((1024, n_elements_per_row)).cuda()\n",
    "    _seeded_matrix_dropout[grid](x, output, debug_random_mask_ptr, n_elements, p, seeds, n_elements_per_row, BLOCK_SIZE=1024)\n",
    "    return output, debug_random_mask_ptr\n",
    "\n",
    "x = torch.randn(size=(3, 4)).cuda() # n elements per row must be a power of 2\n",
    "# Compare this to the baseline - dropout mask is never instantiated!\n",
    "output1, debug_random_mask_ptr1 = seeded_matrix_dropout(x, p=0.75, seeds=torch.rand(x.shape[0]).cuda())\n",
    "output2, debug_random_mask_ptr2 = seeded_matrix_dropout(x, p=0.5, seeds=torch.tensor([3, 5, 2]).cuda())\n",
    "output3, debug_random_mask_ptr3 = seeded_matrix_dropout(x, p=0.5, seeds=torch.tensor([0, 1, 3]).cuda())\n",
    "\n",
    "print(tabulate.tabulate([\n",
    "    [\"input\"] + x.tolist(),\n",
    "    [\"output (first set of seeds)\"] + output.tolist(),\n",
    "    [\"output (2nd set)\"] + output2.tolist(),\n",
    "    [\"output (3rd set)\"] + output3.tolist()\n",
    "]))\n",
    "\n",
    "assert torch.all(torch.eq(debug_random_mask_ptr1, debug_random_mask_ptr2))\n",
    "assert torch.all(torch.eq(debug_random_mask_ptr2, debug_random_mask_ptr3))\n",
    "random_mask_should_be = torch.zeros((1024, x.shape[1])).cuda()\n",
    "random_mask_should_be[1, :] = torch.ones(x.shape[1])\n",
    "assert torch.all(torch.eq(random_mask_should_be, debug_random_mask_ptr3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73a60f90-aaad-40ed-82b3-c4a44dc77aa8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debug_random_mask_ptr1.shape\n",
    "debug_random_mask_ptr1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d9274990-5340-4ec8-a689-135afe4451d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1025 // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6d64a8f2-722c-4220-ad42-cb216d6c6780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512.5"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1025 / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "903c88f0-f4fc-44d3-b261-4186f859a739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6744, 0.3973, 0.4403, 0.8433],\n",
      "        [0.7333, 0.2533, 0.9708, 0.7336],\n",
      "        [0.0207, 0.7678, 0.6073, 0.8185],\n",
      "        [0.7854, 0.7236, 0.6716, 0.3804],\n",
      "        [0.6580, 0.0869, 0.6180, 0.8752],\n",
      "        [0.4964, 0.0204, 0.6187, 0.2358]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-66-db7f5af70017>:8: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  one_hot = ((offsets % block_start) // per_row) == row\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False],\n",
       "        [False, False, False, False],\n",
       "        [False,  True,  True,  True]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 3\n",
    "per_row = 4\n",
    "data = torch.rand(block_size * 2, per_row)\n",
    "# data\n",
    "offsets = torch.arange(block_size * per_row).reshape(block_size, per_row) + block_size * per_row\n",
    "block_start = block_size * per_row\n",
    "row = 2\n",
    "one_hot = ((offsets % block_start) // per_row) == row\n",
    "\n",
    "print(data)\n",
    "random_mask = data[:block_size, :per_row] * one_hot\n",
    "p = 0.5\n",
    "random_mask > p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07bf5e1-b00f-4fb0-912e-c5eedd6f2cb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ef0a36-baf7-480f-a806-b520135d6432",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
