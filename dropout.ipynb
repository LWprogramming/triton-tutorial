{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7697bff8-7d91-43c8-b4fc-bc08920dd171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: triton in ./.local/lib/python3.8/site-packages (2.0.0.post1)\n",
      "Requirement already satisfied: tabulate in ./.local/lib/python3.8/site-packages (0.9.0)\n",
      "Requirement already satisfied: pandas in ./.local/lib/python3.8/site-packages (1.5.1)\n",
      "Requirement already satisfied: matplotlib in ./.local/lib/python3.8/site-packages (3.5.3)\n",
      "Requirement already satisfied: torch in /usr/lib/python3/dist-packages (from triton) (1.12.1)\n",
      "Requirement already satisfied: cmake in ./.local/lib/python3.8/site-packages (from triton) (3.26.1)\n",
      "Requirement already satisfied: lit in ./.local/lib/python3.8/site-packages (from triton) (16.0.0)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from triton) (3.0.12)\n",
      "Requirement already satisfied: numpy>=1.20.3 in ./.local/lib/python3.8/site-packages (from pandas) (1.23.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.local/lib/python3.8/site-packages (from pandas) (2022.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in ./.local/lib/python3.8/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.local/lib/python3.8/site-packages (from matplotlib) (4.38.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/lib/python3/dist-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/lib/python3/dist-packages (from matplotlib) (7.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.6)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.local/lib/python3.8/site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/lib/python3/dist-packages (from matplotlib) (1.0.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas) (1.14.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install triton tabulate pandas matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04add3b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------  ---------  --------  ---------  ----------  --------  -------  ---------  --------  ---------  --------\n",
      "input      -0.396598  0.914868  -0.463365  -0.0863367  0.249196  -1.7128  -0.748133  -1.20401  -0.680449  -1.24109\n",
      "keep mask   1         0          1          0          0          1        1          0         0          1\n",
      "output     -0.793196  0         -0.92673    0          0         -3.4256  -1.49627    0         0         -2.48217\n",
      "---------  ---------  --------  ---------  ----------  --------  -------  ---------  --------  ---------  --------\n"
     ]
    }
   ],
   "source": [
    "# Given in the tutorial. Hardcoded dropout mask.\n",
    "\n",
    "import tabulate\n",
    "import torch\n",
    "\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def _dropout(\n",
    "        x_ptr,  # pointer to the input\n",
    "        x_keep_ptr,  # pointer to a mask of 0s and 1s\n",
    "        output_ptr,  # pointer to the output\n",
    "        n_elements,  # number of elements in the `x` tensor\n",
    "        p,  # probability that an element of `x` is changed to zero\n",
    "        BLOCK_SIZE: tl.constexpr,\n",
    "):\n",
    "    pid = tl.program_id(axis=0)\n",
    "    block_start = pid * BLOCK_SIZE\n",
    "    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n",
    "    mask = offsets < n_elements\n",
    "    # Load data\n",
    "    x = tl.load(x_ptr + offsets, mask=mask)\n",
    "    x_keep = tl.load(x_keep_ptr + offsets, mask=mask)\n",
    "    # The line below is the crucial part, described in the paragraph above!\n",
    "    output = tl.where(x_keep, x / (1 - p), 0.0)\n",
    "    # Write-back output\n",
    "    tl.store(output_ptr + offsets, output, mask=mask)\n",
    "\n",
    "\n",
    "def dropout(x, x_keep, p):\n",
    "    output = torch.empty_like(x)\n",
    "    assert x.is_contiguous()\n",
    "    n_elements = x.numel()\n",
    "    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n",
    "    _dropout[grid](x, x_keep, output, n_elements, p, BLOCK_SIZE=1024)\n",
    "    return output\n",
    "\n",
    "\n",
    "# Input tensor\n",
    "x = torch.randn(size=(10,)).cuda()\n",
    "# Dropout mask\n",
    "p = 0.5\n",
    "x_keep = (torch.rand(size=(10,)) > p).to(torch.int32).cuda()\n",
    "#\n",
    "output = dropout(x, x_keep=x_keep, p=p)\n",
    "print(tabulate.tabulate([\n",
    "    [\"input\"] + x.tolist(),\n",
    "    [\"keep mask\"] + x_keep.tolist(),\n",
    "    [\"output\"] + output.tolist()\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9451619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------  ---------  --------  --------  ---------  ---------  -------  --------  ---------  -------  ----------\n",
      "input                -0.868897  0.403624  -1.51665  -0.749709  -0.951039  1.94522  -2.04819  0.0271463  2.01441  -0.0647303\n",
      "output (seed = 123)   0         0.807247   0         0          0         3.89043   0        0          4.02882  -0.129461\n",
      "output (seed = 123)   0         0.807247   0         0          0         3.89043   0        0          4.02882  -0.129461\n",
      "output (seed = 512)   0         0         -3.0333   -1.49942    0         3.89043  -4.09638  0          0         0\n",
      "-------------------  ---------  --------  --------  ---------  ---------  -------  --------  ---------  -------  ----------\n"
     ]
    }
   ],
   "source": [
    "# Given in the tutorial. random seed controls dropout for a vector\n",
    "@triton.jit\n",
    "def _seeded_dropout(\n",
    "        x_ptr,\n",
    "        output_ptr,\n",
    "        n_elements,\n",
    "        p,\n",
    "        seed,\n",
    "        BLOCK_SIZE: tl.constexpr,\n",
    "):\n",
    "    # compute memory offsets of elements handled by this instance\n",
    "    pid = tl.program_id(axis=0)\n",
    "    block_start = pid * BLOCK_SIZE\n",
    "    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n",
    "    # load data from x\n",
    "    mask = offsets < n_elements\n",
    "    x = tl.load(x_ptr + offsets, mask=mask)\n",
    "    # randomly prune it\n",
    "    random = tl.rand(seed, offsets)\n",
    "    x_keep = random > p\n",
    "    # write-back\n",
    "    output = tl.where(x_keep, x / (1 - p), 0.0)\n",
    "    tl.store(output_ptr + offsets, output, mask=mask)\n",
    "\n",
    "\n",
    "def seeded_dropout(x, p, seed):\n",
    "    output = torch.empty_like(x)\n",
    "    assert x.is_contiguous()\n",
    "    n_elements = x.numel()\n",
    "    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n",
    "    _seeded_dropout[grid](x, output, n_elements, p, seed, BLOCK_SIZE=1024)\n",
    "    return output\n",
    "\n",
    "\n",
    "x = torch.randn(size=(10,)).cuda()\n",
    "# Compare this to the baseline - dropout mask is never instantiated!\n",
    "output = seeded_dropout(x, p=0.5, seed=123)\n",
    "output2 = seeded_dropout(x, p=0.5, seed=123)\n",
    "output3 = seeded_dropout(x, p=0.5, seed=512)\n",
    "\n",
    "print(tabulate.tabulate([\n",
    "    [\"input\"] + x.tolist(),\n",
    "    [\"output (seed = 123)\"] + output.tolist(),\n",
    "    [\"output (seed = 123)\"] + output2.tolist(),\n",
    "    [\"output (seed = 512)\"] + output3.tolist()\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "627a54af-7a4c-4f46-a45d-d9e53d8cc14c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1: dropout for matrix with vector of seeds, 1 seed per row\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "import os\n",
    "# ensure CUDA immediately returns errors so we know it's wrong immediately if something is wrong\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "# Thank you to https://twitter.com/cis_female/ for helping debug this so that it works\n",
    "# now! My original approach was really complex & buggy but you don't actually need\n",
    "# anything more complicated than looping through each row in the block, generating a\n",
    "# new mask for that row.\n",
    "@triton.jit\n",
    "def _seeded_matrix_dropout(\n",
    "        x_ptr,\n",
    "        output_ptr,\n",
    "        debug_random_mask_ptr,\n",
    "        debug_seeds_ptr,\n",
    "        p,\n",
    "        seeds,\n",
    "        n_elements_per_row: tl.constexpr,\n",
    "        BLOCK_SIZE: tl.constexpr,\n",
    "):\n",
    "    # Note: the tl.program_id(0) * BLOCK_SIZE is essential so when writing to a location\n",
    "    # we don't have overwriting (because each block will have its own set of rows\n",
    "    # ranging from 0 to BLOCK_SIZE - 1).\n",
    "    # In general if writing data is inconsistent across runs, it's most likely a\n",
    "    # problem with contention with all the block programs writing to the same spot\n",
    "    # in parallel.\n",
    "    block_offset = tl.program_id(0) * BLOCK_SIZE\n",
    "    # Note that we have block_offset * n_elements_per_row if we're indexing into the\n",
    "    # matrix the shape of x, while just block_offset when indexing into a vector\n",
    "    # the size of x.shape[0] e.g. seeds\n",
    "    current_block_ptr = x_ptr + block_offset * n_elements_per_row\n",
    "    current_block_seeds_ptr = seeds + block_offset\n",
    "    current_block_debug_random_mask_ptr = debug_random_mask_ptr + block_offset * n_elements_per_row\n",
    "    current_block_debug_seeds_ptr = debug_seeds_ptr + block_offset\n",
    "    current_block_output_ptr = output_ptr + block_offset * n_elements_per_row\n",
    "\n",
    "    for row in range(BLOCK_SIZE):\n",
    "        current_row = tl.load(current_block_ptr\n",
    "                              + row * n_elements_per_row\n",
    "                              + tl.arange(0, n_elements_per_row))\n",
    "        rand_seed = tl.load(current_block_seeds_ptr + row)\n",
    "        tl.store(current_block_debug_seeds_ptr + row, rand_seed)\n",
    "        random_mask = tl.rand(rand_seed, tl.arange(0, n_elements_per_row)) < p\n",
    "        tl.store(current_block_debug_random_mask_ptr\n",
    "                 + row * n_elements_per_row\n",
    "                 + tl.arange(0, n_elements_per_row), random_mask)\n",
    "        masked_xs = tl.where(random_mask, current_row, 0.0)\n",
    "        tl.store(current_block_output_ptr + row * n_elements_per_row + tl.arange(0, n_elements_per_row), masked_xs)\n",
    "\n",
    "BLOCK_SIZE = 4 # should be much smaller than your input. e.g. here BLOCK_SIZE is num\n",
    "# rows in a block. Also a previous mistake was to do cdiv based on num elements in\n",
    "# total, not num rows for calculating grid, which came from me copying the grid code\n",
    "# from the matmul tutorial-- oops!\n",
    "def seeded_matrix_dropout(x, p, seeds):\n",
    "    output = torch.empty_like(x)\n",
    "    assert x.is_contiguous()\n",
    "    assert seeds.shape == (x.shape[0],), f\"seeds should be length of num rows but instead got seeds.shape {seeds.shape} and x.shape {x.shape}\"\n",
    "    # BLOCK_SIZE is num rows per block\n",
    "    grid = lambda meta: (triton.cdiv(x.shape[0], meta['BLOCK_SIZE']),)\n",
    "    n_elements_per_row = x.shape[1]\n",
    "    # these debug values should get overwritten so these initial values are just to indicate if it's not being written for some reason\n",
    "    debug_random_mask_ptr = 22 * torch.ones(x.shape, dtype=torch.int).cuda() # random masks used for each row\n",
    "    debug_seeds_ptr = 21 * torch.ones(seeds.shape, dtype=torch.int).cuda()\n",
    "    _seeded_matrix_dropout[grid](x, output, debug_random_mask_ptr, debug_seeds_ptr, p, seeds, n_elements_per_row, BLOCK_SIZE=BLOCK_SIZE)\n",
    "    return output, debug_random_mask_ptr, debug_seeds_ptr\n",
    "\n",
    "x = torch.randn(size=(1000, 64)).cuda() # n elements per row must be a power of 2\n",
    "# Compare this to the baseline - dropout mask is never instantiated!\n",
    "output1, debug_random_mask_ptr1, debug_seeds_ptr1 = seeded_matrix_dropout(x, p=0.75, seeds=torch.rand(x.shape[0]).cuda())\n",
    "# seeds should match for rows 0 and 1, but not for remaining ones\n",
    "seed2 = torch.cat((torch.tensor([30, 50, 100, 4, 41]), torch.tensor(np.random.randint(0, 100, size=(x.shape[0]-5))))).cuda().to(torch.int)\n",
    "seed3 = torch.cat((torch.tensor([30, 50, 100, 4, 41]), torch.tensor(np.random.randint(0, 100, size=(x.shape[0]-5))))).cuda().to(torch.int)\n",
    "assert torch.all(torch.eq(seed2[0:5], seed3[0:5]))\n",
    "assert not torch.all(torch.eq(seed2, seed3))\n",
    "\n",
    "output2, debug_random_mask_ptr2, debug_seeds_ptr2 = seeded_matrix_dropout(x, p=0.5, seeds=seed2)\n",
    "output3, debug_random_mask_ptr3, debug_seeds_ptr3 = seeded_matrix_dropout(x, p=0.5, seeds=seed3)\n",
    "\n",
    "# Check seeds being used correctly\n",
    "# print(seed2[0:10])\n",
    "# print(debug_seeds_ptr2[0:10])\n",
    "# print(seed3[0:10])\n",
    "# print(debug_seeds_ptr3[0:10])\n",
    "assert torch.all(torch.eq(seed2, debug_seeds_ptr2)), f\"seed2 {seed2} debug {debug_seeds_ptr2}\"\n",
    "assert torch.all(torch.eq(seed3, debug_seeds_ptr3)), f\"seed3 {seed2} debug {debug_seeds_ptr3}\"\n",
    "\n",
    "# only the first 5 seeds are hardcoded to be consistently the same\n",
    "# print(f\"debug_random_mask_ptr2{debug_random_mask_ptr2[0:5, :]}\")\n",
    "# print(f\"debug_random_mask_ptr3{debug_random_mask_ptr3[0:5, :]}\")\n",
    "assert torch.all(torch.eq(debug_random_mask_ptr2[0:5, :], debug_random_mask_ptr3[0:5, :])), \"expect mask for first 5 rows to match, because those seeds are hardcoded to be the same\"\n",
    "\n",
    "# first 5 rows should match, but the rest shouldn't!\n",
    "assert torch.all(torch.isclose(output2[0:5, :], output3[0:5, :])), f\"got output2\\n{output2[0:5, :]} and otuput3\\n{output3[0:5, :]} from x\\n{x[0:5, :]}\"\n",
    "assert not torch.all(torch.isclose(output2, output3))\n",
    "print(\"ok, all tests pass\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3289e1a-80b9-4049-93df-8b0f2cc95453",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
