{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4726e1b-5a57-46b5-8c02-71d46c770aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: triton in ./.local/lib/python3.8/site-packages (2.0.0)\n",
      "Requirement already satisfied: matplotlib in ./.local/lib/python3.8/site-packages (3.5.3)\n",
      "Requirement already satisfied: pandas in ./.local/lib/python3.8/site-packages (1.5.1)\n",
      "Requirement already satisfied: tabulate in ./.local/lib/python3.8/site-packages (0.9.0)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from triton) (3.0.12)\n",
      "Requirement already satisfied: lit in ./.local/lib/python3.8/site-packages (from triton) (15.0.7)\n",
      "Requirement already satisfied: torch in /usr/lib/python3/dist-packages (from triton) (1.12.1)\n",
      "Requirement already satisfied: cmake in ./.local/lib/python3.8/site-packages (from triton) (3.25.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.6)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.local/lib/python3.8/site-packages (from matplotlib) (4.38.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/lib/python3/dist-packages (from matplotlib) (1.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.local/lib/python3.8/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/lib/python3/dist-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/lib/python3/dist-packages (from matplotlib) (7.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.local/lib/python3.8/site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.local/lib/python3.8/site-packages (from matplotlib) (1.23.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.local/lib/python3.8/site-packages (from pandas) (2022.5)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.14.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install triton matplotlib pandas tabulate # libraries besides triton for tutorial specifically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b75835e5-c8ff-4f76-8d2a-6e44ef372016",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def naive_softmax(x):\n",
    "    \"\"\"Compute row-wise softmax of X using native pytorch\n",
    "\n",
    "    We subtract the maximum element in order to avoid overflows. Softmax is invariant to\n",
    "    this shift.\n",
    "    \"\"\"\n",
    "    # read  MN elements ; write M  elements\n",
    "    x_max = x.max(dim=1)[0]\n",
    "    # read MN + M elements ; write MN elements\n",
    "    z = x - x_max[:, None]\n",
    "    # read  MN elements ; write MN elements\n",
    "    numerator = torch.exp(z)\n",
    "    # read  MN elements ; write M  elements\n",
    "    denominator = numerator.sum(dim=1)\n",
    "    # read MN + M elements ; write MN elements\n",
    "    ret = numerator / denominator[:, None]\n",
    "    # in total: read 5MN + 2M elements ; wrote 3MN + 2M elements\n",
    "    return ret\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "344165f5-a200-4e67-bffd-8239011196c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def softmax_kernel(\n",
    "    output_ptr, input_ptr, input_row_stride, output_row_stride, n_cols,\n",
    "    BLOCK_SIZE: tl.constexpr\n",
    "):\n",
    "    # The rows of the softmax are independent, so we parallelize across those\n",
    "    row_idx = tl.program_id(0)\n",
    "    # The stride represents how much we need to increase the pointer to advance 1 row\n",
    "    row_start_ptr = input_ptr + row_idx * input_row_stride\n",
    "    # The block size is the next power of two greater than n_cols, so we can fit each\n",
    "    # row in a single block\n",
    "    col_offsets = tl.arange(0, BLOCK_SIZE)\n",
    "    input_ptrs = row_start_ptr + col_offsets\n",
    "    # Load the row into SRAM, using a mask since BLOCK_SIZE may be > than n_cols\n",
    "    row = tl.load(input_ptrs, mask=col_offsets < n_cols, other=-float('inf'))\n",
    "    # Substract maximum for numerical stability\n",
    "    row_minus_max = row - tl.max(row, axis=0)\n",
    "    # Note that exponentials in Triton are fast but approximate (i.e., think __expf in CUDA)\n",
    "    numerator = tl.exp(row_minus_max)\n",
    "    denominator = tl.sum(numerator, axis=0)\n",
    "    softmax_output = numerator / denominator\n",
    "    # Write back output to DRAM\n",
    "    output_row_start_ptr = output_ptr + row_idx * output_row_stride\n",
    "    output_ptrs = output_row_start_ptr + col_offsets\n",
    "    tl.store(output_ptrs, softmax_output, mask=col_offsets < n_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5e989b5-5051-4c7f-806c-97542392f988",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    n_rows, n_cols = x.shape\n",
    "    # The block size is the smallest power of two greater than the number of columns in `x`\n",
    "    BLOCK_SIZE = triton.next_power_of_2(n_cols)\n",
    "    # Another trick we can use is to ask the compiler to use more threads per row by\n",
    "    # increasing the number of warps (`num_warps`) over which each row is distributed.\n",
    "    # You will see in the next tutorial how to auto-tune this value in a more natural\n",
    "    # way so you don't have to come up with manual heuristics yourself.\n",
    "    num_warps = 4\n",
    "    if BLOCK_SIZE >= 2048:\n",
    "        num_warps = 8\n",
    "    if BLOCK_SIZE >= 4096:\n",
    "        num_warps = 16\n",
    "    # Allocate output\n",
    "    y = torch.empty_like(x)\n",
    "    # Enqueue kernel. The 1D launch grid is simple: we have one kernel instance per row o\n",
    "    # f the input matrix\n",
    "    softmax_kernel[(n_rows,)](\n",
    "        y,\n",
    "        x,\n",
    "        x.stride(0),\n",
    "        y.stride(0),\n",
    "        n_cols,\n",
    "        num_warps=num_warps,\n",
    "        BLOCK_SIZE=BLOCK_SIZE,\n",
    "    )\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62ba24ae-c7b1-4e26-988b-b908cca9c172",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "x = torch.randn(1823, 781, device='cuda') # this is the shape\n",
    "y_triton = softmax(x)\n",
    "y_torch = torch.softmax(x, axis=1)\n",
    "assert torch.allclose(y_triton, y_torch), (y_triton, y_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b94a6b7-491a-476b-a621-92fc96567d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "@triton.testing.perf_report(\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=['N'],  # argument names to use as an x-axis for the plot\n",
    "        x_vals=[\n",
    "            128 * i for i in range(2, 100)\n",
    "        ],  # different possible values for `x_name`\n",
    "        line_arg='provider',  # argument name whose value corresponds to a different line in the plot\n",
    "        line_vals=[\n",
    "            'triton',\n",
    "            'torch-native',\n",
    "            'torch-jit',\n",
    "        ],  # possible values for `line_arg``\n",
    "        line_names=[\n",
    "            \"Triton\",\n",
    "            \"Torch (native)\",\n",
    "            \"Torch (jit)\",\n",
    "        ],  # label name for the lines\n",
    "        styles=[('blue', '-'), ('green', '-'), ('green', '--')],  # line styles\n",
    "        ylabel=\"GB/s\",  # label name for the y-axis\n",
    "        plot_name=\"softmax-performance\",  # name for the plot. Used also as a file name for saving the plot.\n",
    "        args={'M': 4096},  # values for function arguments not in `x_names` and `y_name`\n",
    "    )\n",
    ")\n",
    "def benchmark(M, N, provider):\n",
    "    x = torch.randn(M, N, device='cuda', dtype=torch.float32)\n",
    "    if provider == 'torch-native':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.softmax(x, axis=-1))\n",
    "    if provider == 'triton':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: softmax(x))\n",
    "    if provider == 'torch-jit':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: naive_softmax(x))\n",
    "    gbps = lambda ms: 2 * x.nelement() * x.element_size() * 1e-9 / (ms * 1e-3)\n",
    "    return gbps(ms), gbps(max_ms), gbps(min_ms)\n",
    "\n",
    "\n",
    "benchmark.run(show_plots=True, print_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d679475-70d4-4a00-91b6-43c6059c7dad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
